{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Evaluation Lab\n",
    "\n",
    "Complete the exercises below to solidify your knowledge and understanding of supervised learning model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define column names\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', \n",
    "                'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "\n",
    "boston = pd.read_csv(r\"C:\\Users\\adria\\Documents\\GitHub\\w4-nbs\\D4\\files\\housing.csv\", delimiter='\\s+', names=column_names)\n",
    "\n",
    "print(boston.head())\n",
    "\n",
    "# Assuming 'MEDV' is the target column\n",
    "X = boston.drop(columns=['MEDV'])\n",
    "y = boston['MEDV']\n",
    "\n",
    "# Assuming 'boston' is your DataFrame containing the dataset\n",
    "boston = boston.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "# Create DataFrame for features (X) and target (y)\n",
    "#X = boston.drop(columns=['MEDV'])  # Adjust 'target_column_name' to the actual target column\n",
    "#y = boston['MEDV'] \n",
    "\n",
    "# Impute missing values\n",
    "#imputer = SimpleImputer(strategy='mean')\n",
    "#X_imputed = imputer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `MEDV` field represents the median value of owner-occupied homes (in $1000's) and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (404, 13)\n",
      "X_test shape: (102, 13)\n",
      "y_train shape: (404,)\n",
      "y_test shape: (102,)\n"
     ]
    }
   ],
   "source": [
    "#X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def train_regressor_model(model, X, y):\n",
    "    \n",
    "    '''\n",
    "        This function fit a model, and return evaluation of the model based on RMSE, MAE and R2 score.\n",
    "        \n",
    "        Parameters:\n",
    "            model: Model that will be trained\n",
    "            X: DataFrame for train the model\n",
    "            y: Target\n",
    "        Return:\n",
    "            Results of the evaluations the model\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "    r2_score_train = r2_score(y_train, y_pred_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "    r2_score_test = r2_score(y_test, y_pred_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    return print(f'''Model results:   \n",
    "                                      Train r2_score: {r2_score_train}\n",
    "                                      Train rmse: {rmse_train}\n",
    "                                      Train mae: {mae_train}\n",
    "                                      Test r2_score: {r2_score_test} \n",
    "                                      Test rmse: {rmse_test}\n",
    "                                      Test mae: {mae_test}\n",
    "                                      ''')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a `LinearRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming 'boston' is your DataFrame containing the dataset\n",
    "boston = boston.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate and print R-squared for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set R-squared: 0.7508856358979673\n",
      "Testing Set R-squared: 0.6687594935356318\n"
     ]
    }
   ],
   "source": [
    "# Calculate R-squared for the training and testing set, and print\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Set R-squared: {r2_train}\")\n",
    "print(f\"Testing Set R-squared: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate and print mean squared error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set MSE: 21.641412753226312\n",
      "Testing Set MSE: 24.291119474973534\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean squared error for the training and testing set, and print\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Set MSE: {mse_train}\")\n",
    "print(f\"Testing Set MSE: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate and print mean absolute error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set MAE: 3.314771626783226\n",
      "Testing Set MAE: 3.189091965887843\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean absolite error for the training and testing set, and print\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Set MAE: {mae_train}\")\n",
    "print(f\"Testing Set MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "flowers = load_iris()\n",
    "\n",
    "# Create DataFrame for features and target\n",
    "X_2 = pd.DataFrame(flowers.data, columns=flowers.feature_names)\n",
    "y_2 = pd.DataFrame(flowers.target, columns=['class'])\n",
    "\n",
    "\n",
    "flowers = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `class` field represents the type of flower and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_2 shape: (120, 4)\n",
      "X_test_2 shape: (30, 4)\n",
      "y_train_2 shape: (120, 1)\n",
      "y_test_2 shape: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Split the data: 80% for training and 20% for testing\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train_2 shape: {X_train_2.shape}\")\n",
    "print(f\"X_test_2 shape: {X_test_2.shape}\")\n",
    "print(f\"y_train_2 shape: {y_train_2.shape}\")\n",
    "print(f\"y_test_2 shape: {y_test_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a `LogisticRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train_2, y_train_2.values.ravel())\n",
    "\n",
    "# Generate predictions on the training set\n",
    "y_train_pred_2 = logreg.predict(X_train_2)\n",
    "\n",
    "# Generate predictions on the testing set\n",
    "y_test_pred_2 = logreg.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate and print the accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for training set: 0.975\n",
      "Accuracy Score for testing set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy score for the training set\n",
    "accuracy_train = accuracy_score(y_train_2, y_train_pred_2)\n",
    "\n",
    "# Calculate accuracy score for the testing set\n",
    "accuracy_test = accuracy_score(y_test_2, y_test_pred_2)\n",
    "\n",
    "print(f\"Accuracy Score for training set: {accuracy_train}\")\n",
    "print(f\"Accuracy Score for testing set: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate and print the balanced accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score for training set: 0.975609756097561\n",
      "Balanced Accuracy Score for testing set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Calculate balanced accuracy score for the training set\n",
    "balanced_accuracy_train = balanced_accuracy_score(y_train_2, y_train_pred_2)\n",
    "\n",
    "# Calculate balanced accuracy score for the testing set\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test_2, y_test_pred_2)\n",
    "\n",
    "print(f\"Balanced Accuracy Score for training set: {balanced_accuracy_train}\")\n",
    "print(f\"Balanced Accuracy Score for testing set: {balanced_accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calculate and print the precision score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score for training set: 0.9767857142857144\n",
      "Precision Score for testing set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision score for the training set\n",
    "precision_train = precision_score(y_train_2, y_train_pred_2, average='weighted')\n",
    "\n",
    "# Calculate precision score for the testing set\n",
    "precision_test = precision_score(y_test_2, y_test_pred_2, average='weighted')\n",
    "\n",
    "print(f\"Precision Score for training set: {precision_train}\")\n",
    "print(f\"Precision Score for testing set: {precision_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate and print the recall score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score for training set: 0.975\n",
      "Recall Score for testing set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall score for the training set\n",
    "recall_train = recall_score(y_train_2, y_train_pred_2, average='weighted')\n",
    "# Calculate recall score for the testing set\n",
    "recall_test = recall_score(y_test_2, y_test_pred_2, average='weighted')\n",
    "\n",
    "print(f\"Recall Score for training set: {recall_train}\")\n",
    "print(f\"Recall Score for testing set: {recall_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calculate and print the F1 score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for testing set: 1.0\n",
      "F1 Score for training set: 0.9749882794186592\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score for the testing set\n",
    "f1_test = f1_score(y_test_2, y_test_pred_2, average='weighted')\n",
    "# Calculate F1 score for the training set\n",
    "f1_train = f1_score(y_train_2, y_train_pred_2, average='weighted')\n",
    "\n",
    "print(f\"F1 Score for testing set: {f1_test}\")\n",
    "print(f\"F1 Score for training set: {f1_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate confusion matrixs for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for training set:\n",
      "[[40  0  0]\n",
      " [ 0 38  3]\n",
      " [ 0  0 39]]\n",
      "\n",
      "Confusion Matrix for testing set:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrix for the training set\n",
    "conf_matrix_train = confusion_matrix(y_train_2, y_train_pred_2)\n",
    "\n",
    "# Generate confusion matrix for the testing set\n",
    "conf_matrix_test = confusion_matrix(y_test_2, y_test_pred_2)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix for training set:\")\n",
    "print(conf_matrix_train)\n",
    "print(\"\\nConfusion Matrix for testing set:\")\n",
    "print(conf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: For each of the data sets in this lab, try training with some of the other models you have learned about, recalculate the evaluation metrics, and compare to determine which models perform best on each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize Decision Tree and SVC classifiers\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "svc = SVC()\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Define a dictionary to store classifiers\n",
    "classifiers = {\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"SVC\": svc,\n",
    "    \"Random Forest Regressor\": rfr\n",
    "}\n",
    " #mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAKEN FROM CHATGPT, NO CLUE HOW TO DO IT BUT I WANTED TO SEE THE LOGIC BEHIND IT :)\n",
    "\n",
    "**This code will train Decision Tree and SVC classifiers on the Flowers dataset and RandomForestRegressor and HistGradientBoostingRegressor regressors on the Boston dataset. It will then print evaluation metrics for each model on both datasets, allowing for a comparison of their performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Decision Tree on Flowers dataset:\n",
      "Training Accuracy: 1.0, Testing Accuracy: 1.0\n",
      "Training Precision: 1.0, Testing Precision: 1.0\n",
      "Training Recall: 1.0, Testing Recall: 1.0\n",
      "Training F1 Score: 1.0, Testing F1 Score: 1.0\n",
      "Confusion Matrix for Training Set:\n",
      "[[40  0  0]\n",
      " [ 0 41  0]\n",
      " [ 0  0 39]]\n",
      "Confusion Matrix for Testing Set:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "\n",
      "Metrics for SVC on Flowers dataset:\n",
      "Training Accuracy: 0.975, Testing Accuracy: 1.0\n",
      "Training Precision: 0.9752083333333332, Testing Precision: 1.0\n",
      "Training Recall: 0.975, Testing Recall: 1.0\n",
      "Training F1 Score: 0.975003906860447, Testing F1 Score: 1.0\n",
      "Confusion Matrix for Training Set:\n",
      "[[40  0  0]\n",
      " [ 0 39  2]\n",
      " [ 0  1 38]]\n",
      "Confusion Matrix for Testing Set:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "\n",
      "Metrics for Random Forest on Boston dataset:\n",
      "Training MSE: 2.314511554455447, Testing MSE: 9.57907625490196\n",
      "Metrics for HistGradient Boosting on Boston dataset:\n",
      "Training MSE: 2.5566047743592715, Testing MSE: 10.348151473181675\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Iris dataset\n",
    "flowers = load_iris()\n",
    "\n",
    "# Create DataFrame for features and target\n",
    "X_2 = pd.DataFrame(flowers.data, columns=flowers.feature_names)\n",
    "y_2 = pd.DataFrame(flowers.target, columns=['class'])\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = pd.read_csv(r\"C:\\Users\\adria\\Documents\\GitHub\\w4-nbs\\D4\\files\\HousingData.csv\")\n",
    "\n",
    "# Separate features and target for the Boston Housing dataset\n",
    "X_boston = boston.drop(columns=['MEDV'])\n",
    "y_boston = boston['MEDV']\n",
    "\n",
    "# Split the datasets into training and testing sets\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers and regressors\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"SVC\": SVC()\n",
    "}\n",
    "\n",
    "regressors = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"HistGradient Boosting\": HistGradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Iterate through classifiers for Flowers dataset\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_2, y_train_2.values.ravel())\n",
    "    y_pred_train_2 = clf.predict(X_train_2)\n",
    "    y_pred_test_2 = clf.predict(X_test_2)\n",
    "    accuracy_train_2 = accuracy_score(y_train_2, y_pred_train_2)\n",
    "    accuracy_test_2 = accuracy_score(y_test_2, y_pred_test_2)\n",
    "    precision_train_2 = precision_score(y_train_2, y_pred_train_2, average='weighted')\n",
    "    precision_test_2 = precision_score(y_test_2, y_pred_test_2, average='weighted')\n",
    "    recall_train_2 = recall_score(y_train_2, y_pred_train_2, average='weighted')\n",
    "    recall_test_2 = recall_score(y_test_2, y_pred_test_2, average='weighted')\n",
    "    f1_train_2 = f1_score(y_train_2, y_pred_train_2, average='weighted')\n",
    "    f1_test_2 = f1_score(y_test_2, y_pred_test_2, average='weighted')\n",
    "    conf_matrix_train_2 = confusion_matrix(y_train_2, y_pred_train_2)\n",
    "    conf_matrix_test_2 = confusion_matrix(y_test_2, y_pred_test_2)\n",
    "    \n",
    "    print(f\"Metrics for {name} on Flowers dataset:\")\n",
    "    print(f\"Training Accuracy: {accuracy_train_2}, Testing Accuracy: {accuracy_test_2}\")\n",
    "    print(f\"Training Precision: {precision_train_2}, Testing Precision: {precision_test_2}\")\n",
    "    print(f\"Training Recall: {recall_train_2}, Testing Recall: {recall_test_2}\")\n",
    "    print(f\"Training F1 Score: {f1_train_2}, Testing F1 Score: {f1_test_2}\")\n",
    "    print(\"Confusion Matrix for Training Set:\")\n",
    "    print(conf_matrix_train_2)\n",
    "    print(\"Confusion Matrix for Testing Set:\")\n",
    "    print(conf_matrix_test_2)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Iterate through regressors for Boston dataset\n",
    "for name, reg in regressors.items():\n",
    "    # Define a pipeline for preprocessing the data and training the regressor\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "        ('regressor', reg)  # Regressor\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train_boston, y_train_boston)\n",
    "    \n",
    "    # Generate predictions on the training and testing data\n",
    "    y_pred_train_boston = pipeline.predict(X_train_boston)\n",
    "    y_pred_test_boston = pipeline.predict(X_test_boston)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mse_train_boston = mean_squared_error(y_train_boston, y_pred_train_boston)\n",
    "    mse_test_boston = mean_squared_error(y_test_boston, y_pred_test_boston)\n",
    "    \n",
    "    # Print evaluation metrics for the Boston dataset\n",
    "    print(f\"Metrics for {name} on Boston dataset:\")\n",
    "    print(f\"Training MSE: {mse_train_boston}, Testing MSE: {mse_test_boston}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! In the context of machine learning, a regressor is a type of model used for predictive tasks where the target variable is continuous. The goal of regression is to predict a continuous value based on one or more input features.\n",
    "\n",
    "Here's an overview of the regressors used in the provided code:\n",
    "\n",
    "1. **Random Forest Regressor**:\n",
    "   - Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees during training and outputting the mean prediction of the individual trees as the final prediction.\n",
    "   - In the case of RandomForestRegressor, each decision tree in the ensemble is trained on a random subset of the training data with replacement (bootstrap samples), and a random subset of features is considered for each split in the tree.\n",
    "   - RandomForestRegressor is known for its robustness to overfitting, good performance on a wide range of datasets, and ability to capture complex relationships in the data.\n",
    "   \n",
    "2. **HistGradient Boosting Regressor**:\n",
    "   - Histogram-based Gradient Boosting is an advanced boosting algorithm that builds decision trees sequentially, where each new tree corrects errors made by the previous trees.\n",
    "   - Unlike traditional gradient boosting methods that use exact splits, HistGradientBoostingRegressor bins the input features into discrete bins and uses histograms to efficiently compute the best splits for each feature.\n",
    "   - HistGradientBoostingRegressor is known for its scalability, speed, and ability to handle large datasets with high dimensionality.\n",
    "   \n",
    "These regressors are commonly used in practice for a wide range of regression tasks due to their effectiveness, flexibility, and ability to handle complex relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
